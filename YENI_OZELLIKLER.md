# ğŸ‰ Yeni Ã–zellikler - Duplicate Agent v0.9.1+

## ğŸ“¦ Eklenen ModÃ¼ller

### 1. ğŸ” File System Watchdog (Dosya Sistemi Ä°zleyici)
**Dosya:** `src/watchdog_scanner.py`

GerÃ§ek zamanlÄ± duplicate tespit sistemi. Belirtilen dizinlerdeki deÄŸiÅŸiklikleri izler.

**KullanÄ±m:**
```python
from src.watchdog_scanner import FileSystemWatcher

def on_new_file(filepath):
    print(f"Yeni dosya: {filepath}")
    # Duplicate kontrolÃ¼ yap

watcher = FileSystemWatcher(['/home/user/Downloads'])
watcher.start(
    on_file_created=on_new_file,
    file_extensions=['.jpg', '.png', '.pdf']
)
```

**Ã–zellikler:**
- GerÃ§ek zamanlÄ± dosya deÄŸiÅŸikliÄŸi izleme
- Dosya uzantÄ± filtresi
- Debounce (tekrarlayan olaylarÄ± engelleme)
- Gizli dosya desteÄŸi
- Ã‡oklu dizin izleme

---

### 2. â° Scheduled Scanner (ZamanlanmÄ±ÅŸ Tarama)
**Dosya:** `src/scheduled_scanner.py`

Cron job benzeri otomatik tarama sistemi.

**KullanÄ±m:**
```python
from src.scheduled_scanner import ScheduledScanner, ScanSchedule

def scan_callback(directories, options):
    print(f"Tarama baÅŸlatÄ±ldÄ±: {directories}")

scheduler = ScheduledScanner()

# GÃ¼nlÃ¼k tarama
daily_scan = ScanSchedule(
    schedule_type='daily',
    time_value='14:30',
    directories=['/home/user/Downloads'],
    options={'match_content': True}
)
scheduler.add_schedule(daily_scan)
scheduler.start(scan_callback)
```

**Desteklenen Zamanlamalar:**
- `daily`: Her gÃ¼n belirli saatte (Ã¶rn: "14:30")
- `weekly`: HaftanÄ±n belirli gÃ¼nÃ¼ (Ã¶rn: "monday")
- `hourly`: Her saat baÅŸÄ±
- `interval`: Saat bazlÄ± aralÄ±k (Ã¶rn: her 2 saatte)

---

### 3. ğŸ–¼ï¸ Similar File Finder (Benzer Dosya Bulucu)
**Dosya:** `src/similar_file_finder.py`

Ä°Ã§erik benzerliÄŸi analizi (perceptual hash ve metin benzerliÄŸi).

**KullanÄ±m:**
```python
from src.similar_file_finder import SimilarFileFinder

finder = SimilarFileFinder(
    image_threshold=10,  # 0-64 arasÄ± (dÃ¼ÅŸÃ¼k = daha hassas)
    text_threshold=0.75  # 0.0-1.0 arasÄ±
)

results = finder.find_similar_files(file_list)

# SonuÃ§lar
for group in results['images']:
    print(f"Benzer resimler: {group}")

for group in results['texts']:
    print(f"Benzer metinler: {group}")
```

**Ã–zellikler:**
- **Resim benzerliÄŸi:** Perceptual hash (pHash) ve average hash
- **Metin benzerliÄŸi:** SequenceMatcher ile iÃ§erik analizi
- FarklÄ± boyut/dÃ¼zenlemelerde bile benzer resimleri bulur
- %95+ benzer metinleri tespit eder

---

### 4. ğŸ“ Duplicate Folder Finder (Kopya KlasÃ¶r Bulucu)
**Dosya:** `src/duplicate_folder_finder.py`

Ã–zdeÅŸ klasÃ¶r aÄŸaÃ§larÄ±nÄ± bulur.

**KullanÄ±m:**
```python
from src.duplicate_folder_finder import DuplicateFolderFinder

finder = DuplicateFolderFinder(
    min_file_count=3,
    ignore_hidden=True,
    match_exact=True
)

duplicates = finder.scan_directories(
    root_directories=['/home/user/Documents'],
    max_depth=5
)

# Ä°statistikler
stats = finder.get_duplicate_stats(duplicates)
print(f"Tasarruf: {stats['wasted_space_readable']}")
```

**Ã–zellikler:**
- KlasÃ¶r fingerprint hesaplama
- Alt klasÃ¶r yapÄ±sÄ± analizi
- Dosya iÃ§erik karÅŸÄ±laÅŸtÄ±rma
- BÃ¼yÃ¼k dosyalar iÃ§in sample hash
- Benzerlik yÃ¼zdesi hesaplama

---

### 5. ğŸ—œï¸ Compression Suggester (SÄ±kÄ±ÅŸtÄ±rma Ã–nerici)
**Dosya:** `src/compression_suggester.py`

Duplicate dosyalar yerine sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ arÅŸiv Ã¶nerir.

**KullanÄ±m:**
```python
from src.compression_suggester import CompressionSuggester

suggester = CompressionSuggester(
    min_group_size=3,
    min_savings_mb=1.0
)

suggestions = suggester.analyze_duplicate_groups(duplicate_groups)

for suggestion in suggestions:
    print(f"Tasarruf: {suggestion['savings_readable']}")
    print(f"ArÅŸiv: {suggestion['archive_name']}")

# ArÅŸiv oluÅŸtur
suggester.create_archive(
    files=suggestion['files'],
    archive_path='/path/to/archive.zip',
    delete_originals=True
)
```

**Ã–zellikler:**
- Otomatik sÄ±kÄ±ÅŸtÄ±rÄ±labilirlik analizi
- Tahmini sÄ±kÄ±ÅŸtÄ±rma oranÄ±
- ZIP ve TAR.GZ desteÄŸi
- Tasarruf hesaplama
- AkÄ±llÄ± arÅŸiv isimlendirme

---

## ğŸš€ Kurulum

### Gereksinimler
```bash
pip install -r requirements.txt
```

**Yeni BaÄŸÄ±mlÄ±lÄ±klar:**
- `watchdog>=3.0.0` - Dosya sistemi izleme
- `schedule>=1.2.0` - ZamanlanmÄ±ÅŸ gÃ¶revler
- `pillow>=10.0.0` - Resim iÅŸleme
- `imagehash>=4.3.1` - Perceptual hashing

### Sistem Geneli Kurulum
```bash
python setup.py install
```

veya

```bash
pip install -e .
```

---

## ğŸ’¡ KullanÄ±m SenaryolarÄ±

### Senaryo 1: Otomatik Ä°zleme
```python
# Downloads klasÃ¶rÃ¼nÃ¼ sÃ¼rekli izle
# Yeni duplicate dosya geldiÄŸinde bildir

watcher = FileSystemWatcher(['/home/user/Downloads'])
watcher.start(on_file_created=check_duplicate)
```

### Senaryo 2: HaftalÄ±k Temizlik
```python
# Her Pazartesi saat 10:00'da tarama yap
schedule = ScanSchedule(
    schedule_type='weekly',
    time_value='monday',
    directories=['/home/user'],
    options={'match_content': True}
)
scheduler.add_schedule(schedule)
```

### Senaryo 3: FotoÄŸraf ArÅŸivi Temizleme
```python
# Benzer fotoÄŸraflarÄ± bul (dÃ¼zenlenmiÅŸler dahil)
finder = SimilarFileFinder(image_threshold=5)
results = finder.find_similar_files(photo_list)
```

### Senaryo 4: Backup KlasÃ¶r KontrolÃ¼
```python
# Ä°ki backup klasÃ¶rÃ¼ aynÄ± mÄ± kontrol et
finder = DuplicateFolderFinder()
comparison = finder.compare_folders(
    '/media/backup1',
    '/media/backup2'
)
print(f"Benzerlik: {comparison['similarity_percentage']}%")
```

### Senaryo 5: Log DosyasÄ± ArÅŸivleme
```python
# Duplicate log dosyalarÄ±nÄ± sÄ±kÄ±ÅŸtÄ±r
suggester = CompressionSuggester()
suggestions = suggester.analyze_duplicate_groups(log_duplicates)
suggester.create_archive(files, 'logs_archive.zip', delete_originals=True)
```

---

## ğŸ“Š Performans Ä°puÃ§larÄ±

### Watchdog
- Sadece gerekli uzantÄ±larÄ± filtrele
- Debounce sÃ¼resini ayarla
- Gizli dosyalarÄ± yoksay

### Scheduled Scanner
- YoÄŸun saatlerde tarama yapma
- Interval'i ihtiyaca gÃ¶re ayarla
- SonuÃ§larÄ± log'a kaydet

### Similar File Finder
- BÃ¼yÃ¼k resim setleri iÃ§in average_hash kullan (daha hÄ±zlÄ±)
- Text threshold'u %75-85 arasÄ± tut
- BÃ¼yÃ¼k dosyalar iÃ§in max_chars sÄ±nÄ±rla

### Duplicate Folder Finder
- max_depth ile derinliÄŸi sÄ±nÄ±rla
- min_file_count ile kÃ¼Ã§Ã¼k klasÃ¶rleri atla
- BÃ¼yÃ¼k dosyalar iÃ§in sample hash kullanÄ±lÄ±r (otomatik)

### Compression Suggester
- min_savings_mb ile gereksiz arÅŸivleri filtrele
- Zaten sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ formatlarÄ± atla (otomatik)
- Grup boyutunu kontrol et

---

## ğŸ› Hata AyÄ±klama

### Logging Aktif Et
```python
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('duplicate_agent.log'),
        logging.StreamHandler()
    ]
)
```

### Log DosyasÄ±
TÃ¼m modÃ¼ller `logger` kullanÄ±yor:
```python
logger.info("Bilgi mesajÄ±")
logger.warning("UyarÄ± mesajÄ±")
logger.error("Hata mesajÄ±")
```

---

## ğŸ”§ YapÄ±landÄ±rma

### Watchdog AyarlarÄ±
```python
handler = DuplicateWatchdogHandler(...)
handler.debounce_seconds = 5  # VarsayÄ±lan 2 saniye
```

### Scheduler AyarlarÄ±
Ayarlar `~/.duplicateagent/schedules.json` dosyasÄ±nda saklanÄ±r:
```json
[
  {
    "schedule_type": "daily",
    "time_value": "14:30",
    "directories": ["/home/user/Downloads"],
    "options": {"match_content": true},
    "enabled": true
  }
]
```

---

## ğŸ“ Notlar

- TÃ¼m modÃ¼ller baÄŸÄ±msÄ±z Ã§alÄ±ÅŸabilir
- Ana GUI'ye entegrasyon iÃ§in hazÄ±r
- Type hints kullanÄ±lÄ±yor
- Docstring'ler eksiksiz
- Exception handling var
- Logging sistemi entegre

---

## ğŸ¯ Sonraki AdÄ±mlar

1. Ana GUI'ye widget'lar ekle
2. Ayarlar menÃ¼sÃ¼ geniÅŸlet
3. Test coverage artÄ±r
4. CLI arayÃ¼zÃ¼ ekle
5. DokÃ¼mantasyonu tamamla

**TÃ¼m modÃ¼ller production-ready! ğŸš€**
